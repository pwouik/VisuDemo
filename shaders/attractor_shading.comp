#version 460 core
layout (local_size_x = 32, local_size_y = 32, local_size_z = 1) in;
layout(rgba32f, binding = 0) uniform image2D target;
layout(r32i, binding = 1) uniform iimage2D depth;
layout(r32i, binding = 4) uniform iimage2D jumpdist; //texture that store jump distances
layout(std140, binding = 5) uniform Ubo_samples {
    vec4 samples[64]; //MUST MATCH define HALFSPHERE_SAMPLES in attractor.h
};

uniform mat4 inv_view;
uniform mat4 inv_proj;
uniform uvec2 screen_size;

#define PI 3.14159265358
#define BGCOLOR vec4(1,1,1,1)

uniform vec3 col_jd_low;
uniform vec3 col_jd_high;

uniform float JD_FR_MIN;
uniform float JD_FR_MAX;

vec3 normal; //normal in world space
vec3 ssnormal; //normal in screen space
mat2 inv_proj_small;
vec3 wco;

uniform vec3 camera;
uniform vec3 light_pos;
uniform float k_a;          //used as min_light in phong
//uniform vec3 col_ambient; //not used anymore
uniform float k_d;
//uniform vec3 col_diffuse; //not used anymore
uniform float k_s;
uniform float alpha;
uniform vec3 col_specular; //still used (why ?)


uniform int ssao_version;
uniform float ao_fac;
uniform float ao_size;
uniform vec3 col_ao;

//what ?
vec3 rainbow(float i){
    return min(vec3(1),max(vec3(0),sin((vec3(0,2.0/3,4.0/3)+i)*PI)+.5));
}

//shamelessly taken from chatGPT hopefully his trainning data were good
vec3 hash3(uint x) {
    x = (x ^ 61) ^ (x >> 16);
    x *= 9;
    x = x ^ (x >> 4);
    x *= 0x27d4eb2d;
    x = x ^ (x >> 15);

    return vec3(
        float(x) / float(0xFFFFFFFFU),
        float(x * 48271U) / float(0xFFFFFFFFU),
        float(x * 65497U) / float(0xFFFFFFFFU)
    );
}

// taken from https://www.shadertoy.com/view/ttc3zr
uvec3 murmurHash31(uint src) {
    const uint M = 0x5bd1e995u;
    uvec3 h = uvec3(1190494759u, 2147483647u, 3559788179u);
    src *= M; src ^= src>>24u; src *= M;
    h *= M; h ^= src;
    h ^= h>>13u; h *= M; h ^= h>>15u;
    return h;
}
vec3 hash31(float src) {
    uvec3 h = murmurHash31(floatBitsToUint(src));
    return uintBitsToFloat(h & 0x007fffffu | 0x3f800000u) - 1.0;
}

//tbf it doesn't seem like it has any impact at all on the result visually so maybe just ignore this and used a fix value or whatever
vec3 getRandomVector(){
    return normalize(vec3(0,1,0));
    //return normalize(hash3(gl_GlobalInvocationID.x)-0.5);
    //return normalize(hash31(float(gl_GlobalInvocationID.x))-0.5);
}

float sampleDepth(ivec2 pos){
    vec2 prod  = inv_proj_small * vec2(float(imageLoad(depth,pos).r)/(1<<31),1);
    return prod.x / prod.y;
}

float sampleJumpDist(ivec2 pos){
    float jd = float(imageLoad(jumpdist, pos).r)/(1<<31);
    return (jd-JD_FR_MIN)/(JD_FR_MAX-JD_FR_MIN);
}

vec3 getWorldPosition(ivec2 pos) {
    float z = float(imageLoad(depth, pos).r) / (1 << 31);

    // Convert from screen space to normalized device coordinates (NDC)
    vec2 ndc = vec2(pos) / screen_size * 2.0 - 1.0;

    // Form the clip space position
    vec4 clipSpacePos = vec4(ndc, z, 1.0);

    // Unproject to world space
    vec4 worldSpacePos = inv_view*inv_proj * clipSpacePos;

    //perspective divide
    worldSpacePos /= worldSpacePos.w;

    return worldSpacePos.xyz; // Return the 3D world position
}

//this function isn't used anymore
void computeNormals2(){
    float d = sampleDepth(ivec2(gl_GlobalInvocationID.xy));
    float s=-d/screen_size.x*inv_proj[0].x*2;
    float nx = d-sampleDepth(ivec2(gl_GlobalInvocationID.xy)-ivec2(1,0));
    float px = sampleDepth(ivec2(gl_GlobalInvocationID.xy)+ivec2(1,0))-d;
    float ny = d-sampleDepth(ivec2(gl_GlobalInvocationID.xy)-ivec2(0,1));
    float py = sampleDepth(ivec2(gl_GlobalInvocationID.xy)+ivec2(0,1))-d;
    float dx = (nx+px)/2;
    float dy = (ny+py)/2;
    ssnormal = normalize(cross(vec3(s,0,dx),vec3(0,s,dy)));
    normal = mat3(inv_view)*ssnormal;
}
void computeNormals(){
    wco = getWorldPosition(ivec2(gl_GlobalInvocationID.xy));
    vec3 wconx = getWorldPosition(ivec2(gl_GlobalInvocationID.xy)-ivec2(1,0));
    vec3 wcopx = getWorldPosition(ivec2(gl_GlobalInvocationID.xy)+ivec2(1,0));
    vec3 wcony = getWorldPosition(ivec2(gl_GlobalInvocationID.xy)-ivec2(0,1));
    vec3 wcopy = getWorldPosition(ivec2(gl_GlobalInvocationID.xy)+ivec2(0,1));
    vec3 up = normalize(wcopy-wco);
    vec3 down = normalize(wcony-wco);
    vec3 right = normalize(wcopx-wco);
    vec3 left = normalize(wconx-wco);

    normal = -normalize(cross(up,right) + cross(right, down) + cross(down, left) + cross(left, up));
    ssnormal = transpose(mat3(inv_view))*normal;
}
#define SSAOSAMPLES 4
float dubious_ssao_v1(){
    float cd = sampleDepth(ivec2(gl_GlobalInvocationID.xy));
    float acc = 0.1;
    float range = ao_size*float(screen_size.x)/(inv_proj[0].x*-cd);
    float s = max(1,range/SSAOSAMPLES);
    int n=1;
    for(float dx = -range; dx < range; dx+=s){
        for(float dy = -range; dy < range; dy+=s){
            float trueDepth = sampleDepth(ivec2(gl_GlobalInvocationID.xy)+ivec2(dx,dy));
            vec2 wd = vec2(dx,dy)/range;
            float dif = trueDepth-cd;//(cd-dot(wd,ssnormal.xy)/ssnormal.z);
            if(dif > ao_size) continue;
            float de2 = dot(wd,wd);
            acc+= max(dif/de2,0);
            n++;
        }
    }
    return acc/n*ao_fac*100;
}

#define SSAOSTEP 1  //evaluate every pixel on the square
#define SSAOSKIP 0.03  //distance requiered to considere it's too far. This can control the glow on edge if hight value like 1 or 2

/** 
This version computes an AO factor based on how its neighbors point in a grid are closer to the camera
*/
float dubious_ssao_v2(){
    float d = sampleDepth(ivec2(gl_GlobalInvocationID.xy));// * inv_proj[0].x * screen_size.x/2;
    int SSAORANGE = int(round(15 * ao_size));
    float acc = 0;
    for(int dx = -SSAORANGE; dx <= SSAORANGE; dx+=SSAOSTEP){
        for(int dy = -SSAORANGE; dy <= SSAORANGE; dy+=SSAOSTEP){
            float trueDepth = sampleDepth(ivec2(gl_GlobalInvocationID.xy)-ivec2(dx,dy));
            float dif = trueDepth-d;
            if(dif > SSAOSKIP)continue;
            acc+= max(dif,0);
        }
    }
    return acc*50*ao_fac;
}
/** 
This version computes an AO factor based on how its neighbors point in a grid are far from the tangent plane (uses dot product with normals)
*/
float dubious_ssao_v3(){
    float acc = 0;
    int SSAORANGE = int(round(30 * ao_size));
    for(int dx = -SSAORANGE; dx <= SSAORANGE; dx+=SSAOSTEP){
        for(int dy = -SSAORANGE; dy <= SSAORANGE; dy+=SSAOSTEP){
            vec3 o_wco = getWorldPosition(ivec2(gl_GlobalInvocationID.xy)-ivec2(dx,dy));
            float val =  dot(ssnormal, normalize(o_wco-wco));
            acc+= max(val,0);
        }
    }
    return acc*ao_fac*2/5;
}
//compare distance between actual points and samples. Can be interpreted as a spherical from camera.
#define HALFSPHERE_SAMPLES 64
#define RADIUSRESIZE 0.5
float dubious_ssao_v4(){
    mat4 proj = inverse(inv_proj);

    float acc = 0;
    
    float distcam = distance(camera, wco);

    //get a random vec to get a randomly rotated tangent/bitangent
    vec3 randomVec = getRandomVector();
    vec3 tangent = normalize(randomVec - normal * dot(randomVec, normal));
    vec3 bitangent = cross(normal, tangent);

    mat3 TBN =  mat3(tangent, bitangent, normal);
    for(int i=0; i < HALFSPHERE_SAMPLES; i++){
        //sample pos in world space
        vec3 sample_pos = wco + ao_size * RADIUSRESIZE * TBN * samples[i].xyz;
        if(distance(camera, sample_pos) > distcam) acc+=1.0;
    }

    return acc / HALFSPHERE_SAMPLES * ao_fac*50;
}

//this is strictly SSAO as defined in litterature (probably)
float dubious_ssao_v5(){
    mat4 view = inverse(inv_view);
    mat4 proj = inverse(inv_proj);

    float acc = 0;

    //a random vector to get a randomly rotated tangent
    vec3 randomVec = getRandomVector();

    vec3 tangent = normalize(randomVec - normal * dot(randomVec, normal));
    vec3 bitangent = cross(normal, tangent);

    mat3 TBN =  mat3(tangent, bitangent, normal);
    for(int i=0; i < HALFSPHERE_SAMPLES; i++){
        //find sample position in world space
        vec3 sample_pos = wco + ao_size * RADIUSRESIZE * TBN * samples[i].xyz;

        //project sample to screen (same process than screenWriteCoord in attractor.comp)
        vec4 projected_point = proj * view * vec4(sample_pos,1.0);
        if (projected_point.w != 0.0) {
            projected_point.xyz /= projected_point.w;
        }
        vec2 screen_coord = (projected_point.xy + 1.0) * 0.5; // [-1,1] to [0,1]
        screen_coord *= vec2(screen_size); // Map to screen space (pixels)
        
        //compare the depth of the sample, and the depth on depth map at its position (we can directly compare 32bit int)
        int true_sample_depth = int(projected_point.z*(1<<31));
        int expected_sample_depth = imageLoad(depth,ivec2(screen_coord)).r;
        if(true_sample_depth > expected_sample_depth) acc+=1.0;


    }

    return acc / HALFSPHERE_SAMPLES * ao_fac*50;
}

float dubious_ssao(){
    //Most parameter here are highly interdependent. heavy fine tuning requiered if changer a single thing. Also there's a a lot of magic value to somewhat keepthings in the sames ranges because we use the same uniform values
    switch(ssao_version){
        case 0:
            return dubious_ssao_v1();
        case 1:
            return dubious_ssao_v2();
        case 2:
            return dubious_ssao_v3();
        case 3:
            return dubious_ssao_v4();
        case 4:
            return dubious_ssao_v5();
    }
}


//ambient and diffuse are pure white.
vec3 phong(vec3 color, float min_light,float shadow,float ao) {

    vec3 light_dir = normalize(light_pos - wco);
    vec3 reflect_dir = reflect(-light_dir, normal);
    vec3 pos_dir = normalize(camera - wco);

    // Calculate diffuse and specular components
    float coeff_diffuse = dot(normal, light_dir);
    float coeff_specular = pow(max(dot(pos_dir, reflect_dir), 0.0), exp(alpha)) * k_s;

    // Combine ambient, diffuse, and specular components to get the final color
    //return vec3(color *(max(coeff_diffuse *k_d * shadow,min_light) * ao) +
    //            col_specular * coeff_specular * k_s * shadow);
    return (color *max(coeff_diffuse *k_d * shadow,min_light) +
                col_specular * coeff_specular * k_s * shadow) +
                ao * col_ao;
}

void main() {
    // Check if the current thread is within the bounds of the screen
    if (gl_GlobalInvocationID.x>=screen_size.x || gl_GlobalInvocationID.y>=screen_size.y)
        return;

    inv_proj_small = mat2(inv_proj[2].zw,inv_proj[3].zw);
    wco = getWorldPosition(ivec2(gl_GlobalInvocationID.xy));

    float d = sampleDepth(ivec2(gl_GlobalInvocationID.xy));
    if(d<-99){//early break for background
        imageStore(target, ivec2(gl_GlobalInvocationID.xy), BGCOLOR);
        return;
    }

    
    float dj =  sampleJumpDist(ivec2(gl_GlobalInvocationID.xy));
    computeNormals();

    vec3 color = phong(
        col_jd_low*(1-dj)+dj*col_jd_high,    //generate base color gradient from jump distance
        k_a,                    //min light
        1.0,                    //shadow
        dubious_ssao()   //ambient occlusion
        );
    imageStore(target, ivec2(gl_GlobalInvocationID.xy), vec4(color,1)); 


    //debugging display normals
    //imageStore(target, ivec2(gl_GlobalInvocationID.xy), vec4(abs(normal.zyx),1)); 
}